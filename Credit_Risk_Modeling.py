# -*- coding: utf-8 -*-
"""Credit Risk Modeling.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1BA1zVWKvMeFc2NMTl36PMpVCK3rRBYmf

# Credit Risk Modeling

Disusun Oleh : [Mochammad Adhi Buchori](www.linkedin.com/in/mochammad-adhi-b-2049a1136)

## 1. Domain Proyek

Domain proyek yang diambil untuk proyek *machine learning* ini, yaitu **Keuangan** dengan judul **Credit Risk Modeling**.

### 1.1. Latar Belakang

Credit risk atau risiko kredit merupakan salah satu bentuk risiko pasar yang paling banyak dianalisis dan sulit untuk diukur [1]. Risiko ini mengacu pada kemungkinan gagalnya peminjam untuk melunasi pinjaman atau utang mereka kepada pemberi pinjaman atau kreditur. Dalam konteks keuangan, risiko kredit menjadi sangat penting karena dapat mempengaruhi kesehatan finansial suatu institusi atau investor. Hal tersebut disebabkan karena kegagalan peminjam dalam menyelesaikan kewajibannya terkait pembayaran pinjaman dapat menimbulkan konsekuensi finansial yang substansial bagi pemberi pinjaman atau kreditur. Oleh karena itu, pengelolaan risiko kredit menjadi fokus utama dalam industri keuangan untuk memastikan keberlanjutan dan kestabilan operasional.

<div align="center">
  <img src="https://drive.google.com/uc?id=1X37Yt1FYUpteG2EzPUDotMeJmP63wL6y" alt="Credit Risk Modeling">
  <p>Gambar 1.1. Ilustrasi Credit Risk.</p>
</div>

Credit risk melibatkan penilaian risiko kredit dari pihak yang meminjam, termasuk penilaian kredit dan kemampuan perusahaan untuk membayar kembali pinjaman. Faktor-faktor yang mempengaruhi credit risk meliputi keadaan keuangan peminjam, kualitas manajemen, kondisi industri, kondisi ekonomi secara keseluruhan, serta berbagai faktor lain yang dapat mempengaruhi kemampuan peminjam untuk memenuhi kewajiban.

Manajemen credit risk menjadi fokus penting bagi lembaga keuangan dan investor untuk mengelola risiko yang terkait dengan portofolio kredit. Hal ini melibatkan penggunaan berbagai teknik, meliputi analisis kredit yang cermat, diversifikasi portofolio, penetapan batasan kredit, penggunaan instrumen derivatif untuk lindung nilai, dan pemantauan terus-menerus terhadap kualitas kredit secara berkala.

Pembangunan model yang dapat memprediksi credit risk sangat penting dalam konteks industri finansial, terutama untuk perusahaan peminjaman. Dengan adanya model ini, perusahaan dapat melakukan evaluasi yang lebih baik terhadap aplikasi pinjaman yang masuk, sehingga dapat mengurangi risiko default dan meningkatkan kepercayaan dari pihak investor. Model ini juga dapat membantu perusahaan dalam pengambilan keputusan yang lebih tepat dan efisien terkait dengan penentuan bunga, limit kredit, dan kebijakan peminjaman lainnya. Dengan menggunakan data pinjaman yang diterima dan ditolak, model dapat dikembangkan untuk mengidentifikasi pola-pola kredit yang mengarah pada risiko tinggi, sehingga perusahaan dapat mengambil langkah-langkah pencegahan yang diperlukan. Dengan adanya solusi ini, diharapkan perusahaan dapat meningkatkan kinerja operasionalnya dan memberikan layanan yang lebih baik kepada pelanggan serta pemangku kepentingan lainnya.

## 2. Business Understanding

Credit risk adalah probabilitas seorang peminjam akan mengalami kegagalan dalam membayar kembali jumlah pinjaman yang telah diberikan [2]. Dalam implementasinya, pemberian pinjaman dilakukan berdasarkan analisis kemampuan bisnis atau individu untuk memenuhi kewajiban pembayaran di masa mendatang, termasuk pembayaran pokok dan bunga. Dalam implementasinya, pemberi pinjaman melakukan langkah-langkah tertentu untuk memahami kondisi keuangan peminjam dan mengukur risiko bahwa peminjam tidak dapat memenuhi kewajiban pembayaran di masa yang akan datang.

Manajemen credit risk terdiri dari berbagai proses yang melibatkan beberapa langkah. Pada umumnya, proses ini dapat dikategorikan ke dalam 2 (dua) tahap utama, yaitu measurement (pengukuran) dan mitigation (mitigasi) [3]. Dalam hal ini, pengukuran melibatkan evaluasi keuangan dan profil peminjam untuk menilai risiko kredit, sedangkan mitigasi melibatkan penstrukturan pinjaman dan pengendalian portofolio untuk mengurangi risiko kredit. Berikut adalah diagram alur proses bisnis umum untuk mengajukan pinjaman:

<div align="center">
  <img src="https://drive.google.com/uc?id=1-Xk1r159TlSDBR2qqAiLB5OS_fgRmQm0" alt="Diagram Alur Proses Pengajuan Pinjaman">
  <p>Gambar 2.1.  Diagram Alur Proses Pengajuan Pinjaman.</p>
  <p><i>Source : Adapted from</i> [4]</p>
</div>

Berdasarkan gambar di atas, dapat diketahui bahwa secara umum proses pengajuan pinjaman terdiri dari tahapan berikut:

<div>
  <p align="center">Tabel 2.1. Proses Pengajuan Pinjaman.</p>
</div>

| Tahapan | Deskripsi |
|---|---|
| Permohonan Pinjaman | Klien mengajukan permohonan pinjaman dengan mengisi dokumen aplikasi pinjaman. |
| Pengecekan Klien | Sistem memeriksa apakah klien sudah terdaftar di sistem. |
| Dokumen Legal | Klien menyerahkan dokumen legal yang diperlukan. |
| Penilaian Aplikasi | Sistem melakukan penilaian terhadap aplikasi pinjaman berdasarkan dokumen yang diserahkan dan profil klien. |
| Negosiasi | Jika diperlukan, dilakukan negosiasi antara klien dan pihak peminjam terkait persyaratan pinjaman. |
| Persetujuan | Jika aplikasi disetujui, klien akan menerima persetujuan pinjaman dan perjanjian pinjaman. |
| Penandatanganan | Klien menandatangani perjanjian pinjaman. |
| Penyesuaian | Jika diperlukan, dilakukan penyesuaian terhadap jumlah pinjaman dan jadwal pembayaran. |
| Pencairan | Pinjaman dicairkan kepada klien. |
| Pembayaran | Klien melakukan pembayaran pinjaman sesuai dengan jadwal yang telah disepakati. |
| Pelunasan | Pinjaman dianggap lunas setelah seluruh pembayaran diterima. |
| Penolakan | Jika aplikasi ditolak, klien akan menerima pemberitahuan penolakan. |

### 2.1. Problem Statements

Berdasarkan latar belakang dan pemahaman bisnis yang telah diuraikan, proyek ini akan berfokus pada penyelesaian beberapa masalah, meliputi:
1. Bagaimana pendekatan dalam membersihkan data dan preprocessing untuk pemodelan credit risk?
2. Apa variabel penting yang akan dipertimbangkan dalam dataset credit risk dan bagaimana cara menangani data yang hilang?
3. Bagaimana cara menangani ketidakseimbangan kelas dalam dataset saat memilih model credit risk?
4. Bagaimana cara membandingkan pro dan kontra dari model machine learning yang akan digunakan untuk pemodelan credit risk?
5. Apa model machine learning yang paling efektif untuk memprediksi credit risk?

### 2.2. Goals

Tujuan dari proyek ini adalah sebagai berikut:
1. Membangun model machine learning yang dapat digunakan untuk memprediksi credit risk.
2. Membandingkan beberapa algoritma guna memperoleh akurasi terbaik dalam melakukan prediksi terhadap credit risk.
3. Mengidentifikasi variabel yang paling efektif dalam menentukan credit risk.

### 2.3. Solution Statements

Untuk mencapai tujuan yang telah ditetapkan, peneliti mengembangkan model prediktif menggunakan 7 (tujuh) algoritma yang berbeda. Setiap model akan dievaluasi secara komprehensif untuk menentukan model yang paling optimal dalam memprediksi risiko kredit. Berikut adalah algoritma yang akan digunakan dalam pembangunan model prediksi:

1. Random Forest
> Random Forest adalah sebuah algoritma machine learning yang bekerja dengan menggabungkan output dari beberapa decision tree untuk menghasilkan 1 (satu) model prediktif yang optimal [5]. Random Forest terkenal dengan kemudahan penggunaan dan fleksibilitasnya, sehingga banyak diminati.  Algoritma ini dapat menangani permasalahan klasifikasi maupun regresi.
2. Gradient Boosting
> Gradient Boosting adalah metode ensemble machine learning yang digunakan untuk membangun model prediksi secara berurutan dengan menggabungkan ensemble decision tree yang relatif lemah menjadi model yang lebih kuat [6]. Teknik ini bertujuan untuk meningkatkan kinerja prediksi secara keseluruhan dengan mengoptimalkan bobot model berdasarkan kesalahan iterasi sebelumnya. Dengan demikian, kesalahan prediksi berkurang secara bertahap dan akurasi model pun meningkat.
3. AdaBoost
> AdaBoost adalah algoritma ensemble learning yang digunakan untuk meningkatkan kinerja model prediksi dengan menggabungkan beberapa model lemah menjadi model yang lebih kuat [7].
4. XGBoost
> XGBoost adalah toolkit distributed gradient boosting yang telah disesuaikan untuk pelatihan yang efisien dan scalable dari model machine learning [8]. Algoritma ini menggunakan decision trees sebagai base learners dan menerapkan teknik regularisasi untuk meningkatkan generalisasi model [9]. Dikenal karena efisiensi komputasinya, analisis pentingnya variabel, dan penanganan nilai-nilai yang hilang, XGBoost banyak digunakan untuk tugas-tugas seperti regresi, klasifikasi, dan ranking.
5. Logistic Regression
> Logistic Regression adalah algoritma regresi yang digunakan untuk memodelkan probabilitas bahwa suatu data masuk ke dalam kelas tertentu, khususnya ketika variabel dependen bersifat dichotomous (biner) [10]. Meskipun disebut "regresi," Logistic Regression sebenarnya digunakan untuk tugas klasifikasi biner, di mana outputnya adalah probabilitas data termasuk ke dalam kelas target. Algoritma ini menggunakan fungsi sigmoid untuk menghasilkan output yang berada di antara 0 dan 1, yang dapat diinterpretasikan sebagai probabilitas kelas tertentu.
6. K-Nearest Neighbors
> K-Nearest Neighbors adalah algoritma klasifikasi (dan juga regresi) yang bekerja berdasarkan prinsip "majority voting" untuk klasifikasi atau rata-rata dari tetangga terdekat [11]. Algoritma ini menghitung jarak antara data yang ingin diprediksi dengan data training dan memilih kelas mayoritas (atau nilai rata-rata) dari tetangga terdekat (dengan jumlah tetangga yang ditentukan oleh nilai K). KNN merupakan algoritma non-parametrik yang cukup sederhana, namun dapat memberikan hasil yang baik terutama dalam kasus dataset yang relatif kecil dan dimensi yang rendah.
7. Neural Network
> Neural Network (jaringan saraf tiruan) adalah model komputasi yang terinspirasi dari struktur dan fungsi jaringan saraf biologis dalam otak manusia [12]. Tujuan utama Neural Network adalah untuk memproses informasi dan melakukan tugas-tugas seperti klasifikasi, regresi, pengenalan pola, dan lainnya, dengan cara yang mirip dengan cara otak manusia memproses informasi.

## 3. Data Loading

Tahap ini meliputi proses mengimpor atau memuat data untuk analisis atau pemrosesan lebih lanjut. Tujuannya adalah untuk membuat data yang diperlukan tersedia dalam format yang sesuai untuk analisis atau pengolahan lebih lanjut.

> Mengimpor library yang dibutuhkan.
"""

# Google Drive
import os
from google.colab import drive

# DataFrame
import pandas as pd
import numpy as np

# Plotting
import seaborn as sns
import matplotlib.pyplot as plt

# Datetime
import datetime

# Data Preparation
from sklearn.preprocessing import LabelEncoder, OneHotEncoder, MinMaxScaler
from sklearn.compose import ColumnTransformer
from sklearn.model_selection import train_test_split

# Modelling
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
import xgboost as xgb
from sklearn.metrics import accuracy_score
import tensorflow as tf

# Evaluation
from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc

# Mengatur Tema Plotting
sns.set_theme(style="white", palette="pastel")

"""> Menghubungkan Google Drive ke Google Colab"""

drive.mount('/content/gdrive')

"""> Memuat dataset."""

filepath = '/content/gdrive/MyDrive/Help Myself/Rakamin PBI Data Scientist/Assets/Dataset/loan_data_2007_2014.csv'

df = pd.read_csv(filepath, low_memory=False)
df.head()

"""## 4. Data Understanding

<div align="center">
  <img src="https://drive.google.com/uc?id=1nBLWhn90Se7NO935kR93Ql3fIe6ilQTk" alt="Loan Dataset">
  <p>Gambar 4.1. Ilustrasi Loan Dataset.</p>
</div>

Dataset yang digunakan dalam proyek ini, yaitu data pinjaman yang disediakan oleh Rakamin Academy sebagai bagian dari program Project Based Internship. Data tersebut terdiri dari 466.285 entri dengan 75 kolom. Untuk mengakses dataset yang digunakan dalam proyek ini, silakan kunjungi tautan berikut (*Source* : [*Link Dataset*](https://rakamin-lms.s3.ap-southeast-1.amazonaws.com/vix-assets/idx-partners/loan_data_2007_2014.csv)).

> Menampilkan informasi terperinci tentang struktur data pada DataFrame.
"""

df.info()

"""  > Menghitung jumlah nilai NaN dalam setiap kolom."""

df.isna().sum()

"""Berdasarkan informasi struktur data pada DataFrame, terdapat kolom yang seluruhnya berisi "NaN" (Not a Number), yang menandakan ketiadaan data numerik. Selain itu, juga terdapat kolom indeks yang tidak relevan untuk analisis atau pembuatan model. Oleh karena itu, peneliti menghapusnya dengan langkah-langkah berikut:

1. Hapus kolom indeks.
2. Hapus kolom yang hanya berisi nilai NaN atau kosong.

Langkah-langkah ini akan membantu memperbaiki kualitas data dan memastikan bahwa data yang digunakan untuk analisis atau pemodelan lebih bersih dan relevan.

> Menghapus kolom indeks dan kolom yang hanya berisi nilai NaN.
"""

df = df.drop(columns = ['Unnamed: 0'])
df = df.dropna(axis=1, how='all')

"""> Menampilkan informasi terperinci tentang struktur data pada DataFrame."""

df.info()

"""### 4.1. Data Dictionary

Pada tahap ini, kamus data dibuat untuk memahami konteks setiap variabel dalam dataset pinjaman (*loan dataset*). Selain itu, juga dilakukan identifikasi untuk menghapus fitur yang tidak relevan dengan kebutuhan pemodelan. Langkah-langkah ini bertujuan untuk meningkatkan efektivitas dan profesionalisme dalam proses analisis data.

Berikut merupakan kamus data yang disusun untuk membantu memahami konteks setiap variabel dalam *loan dataset*:

> **Important Notes**
1. Variabel yang ditandai dengan huruf **tebal** merupakan variabel yang diasumsikan diperoleh selama **proses pelunasan pinjaman**. Dalam hal ini, fokus analisis adalah pada variabel-variabel yang memiliki hubungan langsung dengan informasi tentang **status pinjaman**.
2. Variabel lain seperti `id`, `member_id`, dan `url` merupakan variabel yang  tidak relevan untuk analisis karena hanya berisikan pengidentifikasi unik atau metadata yang tidak memberikan informasi yang berguna tentang karakteristik atau performa pinjaman.

| Variabel | Deskripsi |
|---|---|
| **id** | ID unik yang ditetapkan LC untuk listing pinjaman. |
| **member_id** | ID unik yang ditetapkan LC untuk anggota peminjam. |
| loan_amnt | Jumlah pinjaman yang diajukan oleh peminjam. Jika departemen kredit mengurangi jumlah pinjaman, nilainya akan tercermin di sini. |
| funded_amnt | Jumlah total yang dikomitkan untuk pinjaman tersebut pada saat itu. |
| funded_amnt_inv | Jumlah total yang dikomitkan oleh investor untuk pinjaman tersebut pada saat itu. |
| term | Jumlah pembayaran pinjaman. Nilai dalam bulan dan bisa 36 atau 60. |
| int_rate | Suku Bunga pinjaman |
| installment | Pembayaran bulanan yang terutang oleh peminjam jika pinjaman berasal. |
| grade | Tingkat pinjaman yang ditetapkan LC |
| sub_grade | Subtingkat pinjaman yang ditetapkan LC |
| emp_title | Jabatan yang diberikan oleh Peminjam saat mengajukan pinjaman. |
| emp_length | Lamanya bekerja dalam tahun. Nilai kemungkinan antara 0 dan 10 dimana 0 berarti kurang dari satu tahun dan 10 berarti sepuluh tahun atau lebih. |
| home_ownership | Status kepemilikan rumah yang diberikan oleh peminjam saat pendaftaran. Nilai yang mungkin: SEWA, MILIK, KPR, LAINNYA. |
| annual_inc | Penghasilan tahunan yang dilaporkan sendiri oleh peminjam saat pendaftaran. |
| verification_status | Menunjukkan apakah pendapatan diverifikasi oleh LC, tidak diverifikasi, atau jika sumber pendapatan diverifikasi. |
| **issue_d** | Bulan dimana pinjaman didanai |
| loan_status | Status pinjaman saat ini |
| pymnt_plan | Menunjukkan apakah rencana pembayaran telah ditetapkan untuk pinjaman |
| **url** | URL untuk halaman LC dengan data listing. |
| desc | Deskripsi pinjaman yang diberikan oleh peminjam |
| purpose | Kategori yang diberikan oleh peminjam untuk permintaan pinjaman. |
| title | Judul pinjaman yang diberikan oleh peminjam |
| zip_code | 3 digit awal dari kode pos yang diberikan oleh peminjam dalam aplikasi pinjaman. |
| addr_state | Negara bagian yang diberikan oleh peminjam dalam aplikasi pinjaman |
| dti | Rasio yang dihitung menggunakan total pembayaran hutang bulanan peminjam pada total kewajiban hutang, tidak termasuk hipotek dan pinjaman LC yang diminta, dibagi dengan pendapatan bulanan yang dilaporkan sendiri oleh peminjam. |
| **delinq_2yrs** | Jumlah kejadian terlambat pembayaran 30+ hari dalam catatan kredit peminjam selama 2 tahun terakhir. |
| earliest_cr_line | Bulan dimana jalur kredit terawal peminjam dibuka |
| **inq_last_6mths** | Jumlah penyelidikan dalam 6 bulan terakhir (tidak termasuk penyelidikan auto dan hipotek) |
| **mths_since_last_delinq** | Jumlah bulan sejak keterlambatan pembayaran terakhir peminjam. |
| **mths_since_last_record** | Jumlah bulan sejak catatan publik terakhir. |
| open_acc | Jumlah jalur kredit terbuka dalam catatan kredit peminjam. |
| pub_rec | Jumlah catatan publik yang merugikan |
| revol_bal | Total saldo kredit bergulir |
| revol_util | Rasio penggunaan kredit bergulir, atau jumlah kredit yang digunakan peminjam relatif terhadap semua kredit bergulir yang tersedia. |
| total_acc | Jumlah total jalur kredit yang saat ini ada di file kredit peminjam |
| initial_list_status | Status pencatatan awal untuk pinjaman. Kemungkinan nilainya adalah - Keseluruhan, Sebagian |
| **out_prncp** | Sisa pokok terutang untuk total keseluruhan yang didanai |
| **out_prncp_inv** | Sisa pokok terutang untuk porsi dari total keseluruhan yang didanai oleh investor |
| **total_pymnt** | Pembayaran yang diterima hingga saat ini untuk total keseluruhan yang didanai |
| **total_pymnt_inv** | Pembayaran yang diterima hingga saat ini untuk porsi dari total keseluruhan yang didanai oleh investor |
| **total_rec_prncp** | Pokok yang diterima hingga saat ini |
| **total_rec_int** | Bunga yang diterima hingga saat ini |
| **total_rec_late_fee** | Biaya keterlambatan yang diterima hingga saat ini |
| **recoveries** | Pemulihan bruto setelah penghapusan buku |
| **collection_recovery_fee** | Biaya pemulihan setelah penghapusan buku |
| **last_pymnt_d** | Bulan terakhir pembayaran diterima |
| **last_pymnt_amnt** | Jumlah total pembayaran terakhir yang diterima |
| **next_pymnt_d** | Tanggal pembayaran terjadwal berikutnya |
| **last_credit_pull_d** | Bulan terakhir LC menarik kredit untuk pinjaman ini |
| **collections_12_mths_ex_med** | Jumlah penagihan dalam 12 bulan tidak termasuk penagihan medis |
| **mths_since_last_major_derog** | Bulan sejak peringkat terburuk 90 hari atau lebih terakhir |
| policy_code | Kode kebijakan yang tersedia untuk umum: policy_code=1 produk baru yang tidak tersedia untuk umum policy_code=2 |
| application_type | Menunjukkan apakah pinjaman tersebut merupakan pengajuan individu atau pengajuan bersama dengan dua peminjam bersama |
| acc_now_delinq | Jumlah akun di mana peminjam saat ini terlambat. |
| **tot_coll_amt** | Total jumlah penagihan yang pernah terutang |
| **tot_cur_bal** | Total saldo saat ini dari semua akun |
| **total_rev_hi_lim** | Total batas kredit/kredit limit tertinggi untuk kredit bergulir |

> Menghapus kolom yang tidak relevan.
"""

irrelevant_features = [
    'id', 'member_id', 'issue_d', 'url', 'delinq_2yrs', 'inq_last_6mths',
    'mths_since_last_delinq', 'mths_since_last_record', 'out_prncp',
    'out_prncp_inv', 'total_pymnt', 'total_pymnt_inv', 'total_rec_prncp',
    'total_rec_int', 'total_rec_late_fee', 'recoveries', 'collection_recovery_fee',
    'last_pymnt_d', 'last_pymnt_amnt', 'next_pymnt_d', 'last_credit_pull_d',
    'collections_12_mths_ex_med', 'mths_since_last_major_derog', 'tot_coll_amt',
    'tot_cur_bal', 'total_rev_hi_lim'
]

df = df.drop(irrelevant_features, axis=1)

"""## 5. Exploratory Data Analysis

Exploratory Data Analysis (EDA) adalah proses investigasi awal yang dilakukan pada dataset untuk memahami dan menganalisis karakteristik utama dalam dataset. Tujuan dari EDA adalah untuk mengidentifikasi pola, hubungan, anomali, dan informasi penting lainnya dalam dataset tanpa membuat asumsi atau hipotesis terlebih dahulu. Metode yang umum digunakan dalam EDA meliputi visualisasi data, statistik deskriptif, dan teknik analisis lainnya untuk mendapatkan pemahaman yang mendalam tentang data sebelum melakukan analisis lebih lanjut atau membangun model prediktif.

### 5.1. Deskripsi Variabel

Tahap ini merujuk pada proses analisis yang bertujuan untuk memahami struktur, karakteristik, dan informasi yang terkandung dalam variabel-variabel yang digunakan dalam suatu dataset.

> Menampilkan informasi terperinci tentang struktur data pada DataFrame.
"""

df.info()

"""> Menghasilkan ringkasan statistik deskriptif dari dataset."""

df.describe()

"""> Melihat informasi tentang kolom-kolom yang memiliki tipe data objek (string)."""

df.select_dtypes(include=object).info()

"""> Menghitung jumlah nilai unik dalam setiap kolom yang memiliki tipe data objek (string)."""

df.select_dtypes(include=object).nunique()

"""> Melihat informasi tentang kolom-kolom yang memiliki tipe data numerik."""

df.select_dtypes(include='number').info()

"""> Menghitung jumlah nilai unik dalam setiap kolom yang memiliki tipe data numerik."""

df.select_dtypes(include='number').nunique()

"""> **Important Inferences**
1. Setelah melakukan analisis variabel, informasi yang diperoleh meliputi jumlah kolom dan baris, tipe data yang digunakan, keberadaan missing value, variasi data (baik kategorikal maupun numerik), serta statistik deskriptif untuk kolom-kolom numerik dalam dataset.
2. Setelah menghapus kolom indeks, menghapus kolom yang hanya berisi nilai NaN atau kosong, dan menghapus irrelevant feature, dataset terdiri dari 31 kolom yang terdiri dari 17 kolom dengan tipe data object, 4 kolom numerik dengan tipe data integer, dan 10 kolom numerik dengan tipe data float.

### 5.2. Feature Engineering

Pada tahap ini, fitur-fitur baru diciptakan dari data yang sudah ada untuk meningkatkan performa model machine learning.

> Mendapatkan nilai unik dari setiap kolom dalam DataFrame.
"""

unique_values = df.apply(lambda x: x.unique())
unique_values

"""Setelah melakukan analisis terhadap isi kolom-kolom yang bersifat unik, peneliti memutuskan untuk menerapkan teknik feature engineering pada kolom `loan_status`. Kolom ini dipilih sebagai target variabel karena berisikan informasi mengenai status pinjaman.

> Menghitung frekuensi kemunculan setiap nilai unik dalam kolom `loan_status`.
"""

df['loan_status'].value_counts()

"""Berdasarkan informasi di atas, dapat disimpulkan bahwa kolom 'loan_status' dalam dataset memiliki beberapa status pinjaman yang termasuk:
1. `Current`: Terdapat 224,226 entri yang menunjukkan pinjaman yang saat ini masih aktif.
2. `Fully Paid`: Terdapat 184,739 entri yang menunjukkan pinjaman yang telah sepenuhnya dilunasi.
3. `Charged Off`: Terdapat 42,475 entri yang menunjukkan pinjaman yang telah dinyatakan sebagai kerugian.
4. `Late (31-120 days)`: Terdapat 6,900 entri yang menunjukkan pinjaman yang telat pembayarannya antara 31 hingga 120 hari.
5. `In Grace Period`: Terdapat 3,146 entri yang menunjukkan pinjaman yang sedang dalam masa penundaan pembayaran.
6. `Does not meet the credit policy. Status:Fully Paid`: Terdapat 1,988 entri yang menunjukkan pinjaman yang selesai namun tidak memenuhi kebijakan kredit.
7. `Late (16-30 days)`: Terdapat 1,218 entri yang menunjukkan pinjaman yang telat pembayarannya antara 16 hingga 30 hari.
8. `Default`: Terdapat 832 entri yang menunjukkan pinjaman yang gagal membayar.
9. `Does not meet the credit policy. Status:Charged Off`: Terdapat 761 entri yang menunjukkan pinjaman yang dinyatakan sebagai kerugian dan tidak memenuhi kebijakan kredit.

Dalam hal ini, beberapa kelas dapat digabungkan menjadi kategori yang lebih umum. Dengan menggabungkan kelas `Fully Paid` dan `Does not meet the credit policy. Status:Fully Paid` menjadi satu kategori yang positif, serta menggabungkan kelas `Charged Off` dan `Default` menjadi satu kategori yang negatif, dua kategori yang lebih mudah dipahami dan diinterpretasikan dapat dihasilkan. Hal ini membantu menyederhanakan model klasifikasi, meningkatkan kinerja dengan mengurangi kompleksitas, mengurangi risiko overfitting, serta meningkatkan interpretasi hasil.

> **Important Notes**
* Kedua pasangan kelas dapat digabungkan karena keduanya mencerminkan hasil yang serupa. `Fully Paid` dan `Does not meet the credit policy. Status:Fully Paid` menunjukkan pinjaman telah dilunasi sepenuhnya, sementara `Charged Off` dan `Default` menunjukkan pinjaman tidak dilunasi.

> `Lunas`: Terdiri dari kategori `Fully Paid` dan `Does not meet the credit policy. Status:Fully Paid`.
"""

fully_paid_classes = ['Fully Paid', 'Does not meet the credit policy. Status:Fully Paid']
df_fully_paid = df[df['loan_status'].isin(fully_paid_classes)]

"""> Menghitung jumlah total entri dalam kolom `loan_status` dari DataFrame `df_fully_paid`"""

df_fully_paid['loan_status'].value_counts().sum()

"""Terdapat 186.727 entri yang menunjukkan pinjaman yang telah sepenuhnya dilunasi.

> Membuat bar plot yang menunjukkan jumlah pinjaman untuk setiap status pinjaman `loan_status` di mana pinjaman lunas.
"""

plt.figure(figsize=(10, 5))

count = df_fully_paid['loan_status'].value_counts()
ax = sns.barplot(x=count.index, y=count.values, hue=count.index, legend=False)

for i, container in enumerate(ax.containers):
    ax.bar_label(container, fontsize=10)

ax.set_title('Loan Status Lunas')
plt.xlabel('Loan Status')
plt.ylabel('Total')

plt.show()

"""> `Late`: Terdiri dari kategori `Late (31-120 days)` dan `Late (16-30 days)`. Terdapat total 8,118 entri yang mencakup keterlambatan pembayaran, yang dapat digabungkan menjadi satu kategori."""

charged_off_classes = ['Charged Off', 'Default']
df_charged_off = df[df['loan_status'].isin(charged_off_classes)]

"""> Menghitung jumlah total entri dalam kolom `loan_status` dari DataFrame `df_charged_off`"""

df_charged_off['loan_status'].value_counts().sum()

"""Terdapat 43.307 entri yang menunjukkan pinjaman yang tidak dilunasi.

> Membuat bar plot yang menunjukkan jumlah pinjaman untuk setiap status pinjaman `loan_status` di mana tidak dilunasi.
"""

plt.figure(figsize=(5, 5))

count = df_charged_off['loan_status'].value_counts()
ax = sns.barplot(x=count.index, y=count.values, hue=count.index, legend=False)

for i, container in enumerate(ax.containers):
    ax.bar_label(container, fontsize=10)

ax.set_title('Loan Status Tidak Lunas')
plt.xlabel('Loan Status')
plt.ylabel('Total')

plt.show()

"""Dengan menggabungkan kategori-kategori ini, kini dataset memiliki beberapa kelas yang mencerminkan status pinjaman secara lebih umum.

> Mengganti nilai untuk `Fully Paid` dan `Does not meet the credit policy. Status:Fully Paid`menjadi `Fully Paid`.
"""

df['loan_status'] = np.where(df['loan_status'].isin(["Fully Paid",
                                                     "Does not meet the credit policy. Status:Fully Paid"
                                                     ]),
                             "Fully Paid",
                             df['loan_status'])

"""> Mengganti nilai untuk `Charged Off` dan `Default` menjadi `Charged Off`."""

df['loan_status'] = np.where(df['loan_status'].isin(["Charged Off",
                                                     "Default"
                                                     ]),
                             "Charged Off",
                             df['loan_status'])

"""> Menyaring DataFrame sehingga hanya mempertahankan baris yang memiliki nilai `Fully Paid` dan `Charged Off` dalam kolom `loan_status`."""

df = df[df['loan_status'].isin(["Fully Paid", "Charged Off"])]

"""> Membuat bar plot yang menunjukkan jumlah pinjaman untuk setiap status pinjaman `loan_status`."""

plt.figure(figsize=(15, 5))

count = df['loan_status'].value_counts()
ax = sns.barplot(x=count.index, y=count.values, hue=count.index, legend=False)

for i, container in enumerate(ax.containers):
    ax.bar_label(container, fontsize=10)

ax.set_title('Loan Status Keseluruhan')
plt.xlabel('Loan Status')
plt.ylabel('Total')

plt.show()

"""> Menghitung jumlah kemunculan setiap nilai dalam kolom `loan_status`."""

df['loan_status'].value_counts()

"""### 5.3. Data Analysis

#### 5.3.1. Univariate Analysis

Univariate Analysis adalah sebuah metode analisis statistik yang digunakan untuk memahami karakteristik dari satu variabel tunggal dalam suatu dataset. Tujuan utama dari analisis univariat adalah untuk merangkum dan menyajikan data, serta mendapatkan wawasan yang lebih dalam tentang distribusi, pola, dan sifat-sifat statistik dari variabel.

##### 5.3.1.1. Categorical Features

###### `application_type`

> Menghapus kolom `application_type` karena hanya memiliki satu nilai unik.
"""

df = df.drop(['application_type'], axis=1)

"""###### `emp_title`

> Menampilkan 10 baris pertama dari kolom `emp_title`.
"""

df['emp_title'].head(10)

"""> Menghitung jumlah nilai-nilai yang hilang (NaN) dalam kolom `emp_title`."""

df['emp_title'].isna().sum()

"""Berdasarkan informasi pada tahap Deskripsi Variabel, dapat diamati bahwa variabel `emp_title` memiliki sejumlah besar nilai unik. Selain itu, variabel `emp_title` juga mempunyai nilai NaN yang banyak sebesar 12.867. Terlalu banyak kategori dan terlalu banyak nilai NaN dapat menyebabkan kebingungan selama pelatihan model. Oleh karena itu, peneliti memutuskan untuk menghapus kolom `emp_title`.

> Menghapus kolom `emp_title` karena memiliki terlalu banyak nilai unik, yakni sebesar 12.867.
"""

df = df.drop(['emp_title'], axis=1)

"""###### `earliest_cr_line`

> Melakukan pengecekkan pada kolom `earliest_cr_line `.
"""

df['earliest_cr_line'].head(10)

"""> Mengubah kolom `earliest_cr_line` menjadi tipe data datetime. Kemudian menghasilkan deskripsi statistik dari kolom tersebut."""

df['earliest_cr_line'] = pd.to_datetime(df['earliest_cr_line'], format='%b-%y')
df['earliest_cr_line'].describe(datetime_is_numeric=True)

"""> Memperbaiki tanggal yang disimpan dalam format datetime."""

def fix_date(x):
    if pd.isnull(x):
        return np.nan
    if x.year > 2030:
        year = x.year - 100
    else:
        year = x.year
    return datetime.date(year, x.month, x.day)

df['earliest_cr_line'] = df['earliest_cr_line'].apply(fix_date)

"""> Memberikan ringkasan statistik terkait kolom dengan tipe data datetime."""

df['earliest_cr_line'].describe(datetime_is_numeric=True)

"""> Format data dalam kolom `earliest_cr_line` dari datetime menjadi hanya tahun."""

df['earliest_cr_line'] = pd.DatetimeIndex(df['earliest_cr_line']).year

"""> Menghitung jumlah nilai unik dalam kolom `earliest_cr_line`."""

df['earliest_cr_line'].nunique()

"""> Menampilkan distribusi tanggal pada kolom `earliest_cr_line`."""

sns.histplot(data=df, x="earliest_cr_line", hue="loan_status", bins=65)

plt.show()

"""##### 5.3.1.2. Numerical Features

> Memberikan ringkasan statistik deskriptif pada kolom `loan_amnt`.
"""

df['loan_amnt'].describe()

"""> Menghasilkan histogram untuk setiap kolom dalam DataFrame terhadap daata yang bersifat numerik."""

df.hist(bins=50, figsize=(20, 15))
plt.show()

"""> **Important Inferences**  
Dalam hal ini, peneliti berfokus pada variabel `loan_amnt` karena berdasarkan informasi dalam Data Dictionary yang tersedia, perubahan dalam jumlah pinjaman akan tercermin pada variabel tersebut. Dengan demikian, berdasarkan informasi yang dihasilkan dari ringkasan statistik deskriptif pada kolom `loan_amnt` dan visualisasi histogram untuk variabel numerik, dapat disimpulkan bahwa:
1. Rata-rata pinjaman adalah Rp14.317.277,57 dengan standar deviasi Rp. 8.286.509,16.
2. Sebagian besar pinjaman berada di antara Rp8.000.000 dan Rp20.000.000.
3. Nilai maksimum jumlah pinjaman, yakni sebesar Rp35.000.000, sedangkan nilai minimum jumlah pinjaman, yakni sebesar Rp500.000.
4. Distribusi pinjaman tidak simetris.

#### 5.3.2. Multivariate Analysis

Multivariate Analysis adalah sebuah pendekatan statistik yang digunakan untuk memahami hubungan antara dua atau lebih variabel dalam sebuah dataset. Berbeda dengan analisis univariat yang hanya fokus pada satu variabel tunggal, analisis multivariat memungkinkan pengguna untuk mengeksplorasi korelasi, pola, dan struktur yang kompleks antara beberapa variabel.

##### 5.3.2.1. Categorical Features

###### `desc`, `purpose`, and `title`

> Melakukan pengecekkan pada kolom `desc`, `purpose`, and `title`.
"""

df[['desc', 'purpose', 'title']].head(10)

"""Apabila dilakukan peninjauan pada variabel `desc`, terdapat proporsi yang cukup besar dari deskripsi yang kosong. Oleh karena itu, peneliti mencoba mengekstrak variabel ini untuk memastkan apakah `desc` memiliki nilai NaN.

> Menambahkan kolom baru bernama `desc_is_na` ke dalam DataFrame.
"""

df['desc_is_na'] = df['desc'].isna()

"""> Menghitung jumlah kemunculan (*count*) dari setiap nilai dalam kolom `desc_is_na`."""

df['desc_is_na'].value_counts()

"""> Membuat dua plot yang terdiri dari countplot yang menunjukkan jumlah kemunculan setiap kategori `desc_is_na` dengan membedakan warna berdasarkan `loan_status` dan stacked bar plot yang menunjukkan distribusi `loan_status` terhadap nilai `desc_is_na`."""

fig, axs = plt.subplots(ncols=2, figsize=(15, 5))
sns.countplot(df, x='desc_is_na', hue='loan_status', ax=axs[0])
df.groupby('desc_is_na')['loan_status'].value_counts(normalize=True).unstack('loan_status').plot.bar(stacked=True, ax=axs[1])

plt.show()

"""Apabila dilihat melalui plot count di atas, terlihat bahwa deskripsi yang telah diisi (False) memiliki proporsi status pinjaman yang lunas (Fully Paid) lebih tinggi. Lalu, jumlah nilai True (139.844) menunjukkan bahwa sebagian besar entri tidak memiliki deskripsi (`desc`), sedangkan jumlah nilai False (90.190) menunjukkan bahwa terdapat sejumlah entri yang memiliki deskripsi. Karena data dalam kolom `desc` tidak lengkap dan informasi yang terdapat pada kolom `desc` tidak relevan dengan pembuatan model (mengacu pada Data Dictionary), peneliti memutuskan untuk menghapus kolom `desc`.

> Menghapus kolom `desc`.
"""

df = df.drop('desc', axis=1)

"""> Menghitung jumlah nilai unik yang muncul dalam kolom `title` dan `purpose`. Kemudian mengambil sepuluh kombinasi nilai unik yang paling umum."""

df[['title', 'purpose']].value_counts()[:10]

"""Jika dilihat berdasarkan hasil dari perhitungan nilai di atas, dapat disimpulkan bahwa variabel `purpose` secara jelas diturunkan dari `title`. Oleh karena itu, `title` menjadi suatu variabel yang redundan dan telah dirangkum menjadi `purpose`.

> Menghapus kolom `title`.
"""

df = df.drop('title', axis=1)

"""###### `zip_code ` dan `addr_state`

> Melakukan pengecekkan pada kolom `zip_code ` dan `addr_state`.
"""

df[['zip_code', 'addr_state']].head(5)

"""> Menghitung jumlah kemunculan (*count*) kombinasi nilai dari dua kolom, yaitu `zip_code` dan `addr_state`."""

df[['zip_code', 'addr_state']].value_counts()[:20]

"""> Mengambil 20 nilai teratas dari hasil perhitungan jumlah dari kolom `addr_state`."""

df['addr_state'].value_counts()[:20].index.to_numpy()

"""> Membuat diagram batang yang menunjukkan jumlah pinjaman yang disetujui dan ditolak untuk 20 negara bagian teratas."""

top_20_state = ['CA', 'NY', 'TX', 'FL', 'IL', 'NJ', 'PA', 'OH', 'GA', 'VA',
                'NC', 'MI', 'MA', 'MD', 'AZ', 'WA', 'CO', 'MN', 'MO', 'CT']

df_plot = df[df['addr_state'].isin(top_20_state)]
df_plot = df_plot.groupby(['loan_status', 'addr_state']).size().reset_index().pivot(columns='loan_status', index='addr_state', values=0)
df_plot.loc[top_20_state].plot(kind='bar', stacked=True)

plt.show()

"""Dapat diketahui bahwa 3 (tiga) angka pertama menunjukkan *sectional center* (pusat seksional) dan juga mencakup informasi negara bagian. Oleh karena itu, terdapat informasi yang redundan antara variabel `zip_code` dengan `addr_state` karena variabel `addr_state` hanya menjelaskan negara bagian dari tiga digit pertama dalam kode pos.

> **Important Notes**  
* *Sectional center* (pusat seksional) merupakan fasilitas pos utama di suatu wilayah yang bertanggung jawab atas pengelolaan dan distribusi pos di wilayah tersebut.

> Menghapus kolom `zip_code`.
"""

df = df.drop('zip_code', axis=1)

"""###### `grade` and `sub_grade`

> Menampilkan nilai unik yang terdapat dalam kolom `grade`.
"""

print(df['grade'].unique())

"""> Menampilkan nilai unik yang terdapat dalam kolom `sub_grade`."""

print(df['sub_grade'].unique())

fig, axs = plt.subplots(ncols=2, figsize=(20, 5))
pd.crosstab(df['grade'], df['loan_status']).plot(kind='bar', ax=axs[0])
pd.crosstab(df['sub_grade'], df['loan_status']).plot(kind='bar', stacked=True, ax=axs[1])

plt.show()

"""> Membuat tabel silang antara kolom `grade` dan `loan_status` dan menormalisasi berdasarkan baris ('index')."""

pd.crosstab(df['grade'], df['loan_status'], normalize='index').round(2)

"""> Membuat tabel silang antara kolom `sub_grade` dan `loan_status` dan menormalisasi berdasarkan baris ('index')."""

pd.crosstab(df['sub_grade'], df['loan_status'], normalize='index').round(2)

"""Berdasarkan informasi di atas, dapat disimpulkan bahwa setiap `sub_grade` mencerminkan `grade` sehingga variabel `grade` dapat dihapus untuk mengurangi redudansi informasi.

> Menghapus kolom `grade`.
"""

df = df.drop('grade', axis=1)

"""##### 5.3.2.2. Numerical Features

Dalam analisis variabel numerik, langkah yang umum dilakukan adalah mengadakan analisis korelasi untuk mengevaluasi tingkat hubungan antar variabel. Tujuan utamanya adalah untuk mengidentifikasi ketergantungan antar variabel, mengevaluasi tingkat redundansi, dan menentukan korelasi antar variabel yang dapat mempengaruhi independensi.

###### `loan_amnt`, `funded_amnt`, `funded_amnt_inv`, and `policy_code`.

> Membuat sebuah plot heatmap yang menampilkan matriks korelasi antar variabel.
"""

plt.subplots(figsize=(20, 10))
sns.heatmap(df.corr(numeric_only=True), vmin=-1, vmax=1, annot=True, cmap="winter")

plt.show()

"""Dapat diamati bahwa variabel `loan_amnt`, `funded_amnt`, dan `funded_amnt_inv` memiliki nilai korelasi yang sempurna. Hal ini menunjukkan dengan jelas bahwa variabel-variabel tersebut redundan. Oleh karena itu, peneliti memutuskan untuk hanya menggunakan variabel `loan_amnt` karena berdasarkan informasi dalam Data Dictionary yang tersedia, perubahan dalam jumlah pinjaman akan tercermin pada variabel tersebut. Selain itu, terdapat pula variabel yang hanya memiliki satu nilai, yaitu `policy_code`. Kondisi ini menyebabkan variabel tersebut tidak relevan dalam konteks korelasi dengan variabel lainnya.

> Menghapus variabel yang redundan dan tidak relevan.
"""

df = df.drop(['funded_amnt', 'funded_amnt_inv', 'policy_code'], axis=1)

"""###### `loan_amnt` and `installment`

Selanjutnya, terdapat variabel yang menunjukkan korelasi yang signifikan, yakni antara `loan_amnt` dengan `installment`. Oleh karena itu, peneiti memutuskan untuk mengeksplorasi kedua variabel ini guna mengidentifikasi kemungkinan adanya informasi yang duplikat di antara keduanya, serta korelasi keduanya dengan variabel target.

> **Important Notes**
1. `loan_amnt` : Jumlah pinjaman yang terdaftar yang diajukan oleh peminjam. Jika pada suatu waktu departemen kredit mengurangi jumlah pinjaman, maka hal tersebut akan tercermin dalam nilai ini.
2. `installment` : Pembayaran bulanan yang harus dibayarkan oleh peminjam jika pinjaman disetujui.

> Membuat subplot dengan dua boxplot secara horizontal. Boxplot pertama menampilkan distribusi `loan_amnt` berdasarkan `loan_status`, sedangkan boxplot kedua menampilkan distribusi `installment` berdasarkan `loan_status`.
"""

fig, axs = plt.subplots(ncols=2, figsize=(25, 10))
sns.boxplot(x='loan_status', y='loan_amnt', data=df, ax=axs[0], hue='loan_status', legend=False)
sns.boxplot(x='loan_status', y='installment', data=df, ax=axs[1], hue='loan_status', legend=False)

plt.show()

"""> Membuat subplot dengan dua histogram secara horizontal. Histogram pertama menampilkan distribusi `loan_amnt` berdasarkan `loan_status`, sedangkan histogram kedua menampilkan distribusi `installment` berdasarkan `loan_status`."""

fig, axs = plt.subplots(ncols=2, figsize=(25, 10))
sns.histplot(data=df, x="loan_amnt", hue="loan_status", bins=11, ax=axs[0])
sns.histplot(data=df, x="installment", hue="loan_status", bins=50, ax=axs[1])

plt.show()

"""Berdasarkan hasil obeservasi terhadap distribusi kedua variabel ini (`loan_amnt` dan `installment`), tidak terlihat adanya redundansi informasi karena `installment` merupakan informasi mengenai jumlah yang harus dibayar oleh klien setiap bulan berdasarkan `loan_amnt`, `term`, dan `int_rate`. Oleh karena itu, kedua variabel ini masih layak untuk digunakan agar model memiliki interpretabilitas yang baik, mengingat `installment` memiliki keterkaitan dengan karakteristik pinjaman. Variabel ini juga dapat dijadikan pertimbangan dalam melakukan eksperimen dengan model untuk memaksimalkan kualitas pemodelan.

## 6. Data Preparation

Data Preparation adalah proses persiapan data sebelum data dapat digunakan untuk analisis atau pemodelan. Tujuannya adalah untuk memastikan data siap digunakan dalam analisis atau pemodelan. Berikut merupakan teknik yang peneliti gunakan pada tahap Data Preparation:
1. Handle Missing Data
2. Convert Numeric Variable
3. Remove Outliers
4. Encode Data
5. Train Test Split

### 6.1. Handle Missing Data

Pada tahap ini, dilakukan identifikasi, analisis, dan pembersihan nilai kosong dalam dataset untuk memastikan konsistensi dan keandalan data yang digunakan dalam analisis atau pemodelan. Langkah-langkah ini bertujuan untuk memastikan kualitas dan akurasi data yang digunakan dalam proses selanjutnya.

> Memeriksa setiap kolom dalam DataFrame. Jika terdapat nilai yang hilang (NaN) dalam kolom tersebut, maka kode akan mencetak nama kolom, jumlah nilai yang hilang, dan persentase nilai yang hilang dari keseluruhan data.
"""

for column in df.columns:
    if df[column].isna().sum() != 0:
        missing = df[column].isna().sum()
        missing_prob = (missing / df.shape[0]) * 100
        print(f"{column}: {missing}({missing_prob:.2f}%)")

"""#### `emp_length`

> Membuat tabel silang (crosstab) antara kolom `emp_length` dan `loan_status`.
"""

pd.crosstab(df['emp_length'], df['loan_status'], normalize='index').round(2)

"""Dapat dilihat bahwa proporsi `loan_status` antara `emp_length` sangat serupa. Sebelum mengambil keputusan untuk menghapus kolom tersebut atau tidak, perlu untuk melihat komposisi dari `emp_length` yang memiliki nilai NaN.

>  Melihat komposisi dari `emp_length` yang memiliki nilai NaN.
"""

df[df['emp_length'].isnull()]['loan_status'].value_counts()

"""Berdasarkan informasi di atas, `emp_length` yang memiliki nilai NaN juga mengandung banyak informasi dari klien (Fully Paid dan Charged Off). Oleh karena itu, tidak bisa hanya menghapus baris yang memiliki nilai `emp_length` NaN karena akan mengurangi banyak informasi dari klien (Fully Paid dan Charged Off). Oleh karena itu, fitur emp_length sebaiknya dihapus.

> Menghapus kolom `emp_length` dari DataFrame.
"""

df = df.drop('emp_length', axis=1)

"""#### `annual_inc`, `earliest_cr_line`, `open_acc`, `pub_rec`, `revol_util`, `total_acc`, `acc_now_delinq`

> Melihat komposisi dari `annual_inc`, `earliest_cr_line`, `open_acc`, `pub_rec`, `revol_util`, `total_acc`, `acc_now_delinq` yang memiliki nilai NaN.
"""

columns_to_check = ['annual_inc', 'earliest_cr_line', 'open_acc', 'pub_rec', 'revol_util', 'total_acc', 'acc_now_delinq']

for column in columns_to_check:
    print(f"Komposisi dari kolom {column} yang memiliki nilai NaN:")
    print(df[df[column].isnull()]['loan_status'].value_counts())
    print("")

"""Berdasarkan informasi komposisi data di atas, jumlah data dalam kolom `annual_inc`, `earliest_cr_line`, `open_acc`, `pub_rec`, `revol_util`, `total_acc`, `acc_now_delinq` yang memiliki nilai NaN cukup kecil. Oleh karena itu, peneliti menghapus baris yang mengandung nilai NaN di seluruh kolom.

> Menyaring DataFrame untuk hanya mempertahankan baris-baris di mana kolom `annual_inc`, `earliest_cr_line`, `open_acc`, `pub_rec`, `revol_util`, `total_acc`, `acc_now_delinq` memiliki nilai yang tidak bernilai NaN.
"""

cols = ['annual_inc', 'earliest_cr_line', 'open_acc', 'pub_rec', 'revol_util', 'total_acc', 'acc_now_delinq']
for col in cols:
  df = df[df[col].notnull()]

"""### 6.2. Convert Numeric Variables

Pada tahap ini, dilakukan pemeriksaan dan penyesuaian tipe data kolom-kolom dalam DataFrame df untuk memastikan data dapat diproses sesuai dengan kebutuhan analisis atau pemodelan yang akan dilakukan. Langkah ini penting untuk memastikan konsistensi dan akurasi data yang digunakan.

> Memberikan informasi tentang kolom-kolom dalam DataFrame yang memiliki tipe data numerik.
"""

df.select_dtypes(include=np.number).info()

"""> Memberikan informasi tentang kolom-kolom dalam DataFrame yang memiliki tipe data boolean."""

df.select_dtypes(include=bool).info()

"""> Mengubah tipe data kolom tertentu dalam DataFrame df menjadi tipe data yang sesuai dengan kebutuhan pemodelan.
1. `col_to_int` adalah daftar kolom yang akan diubah menjadi tipe data integer.
2. `col_to_float` adalah daftar kolom yang akan diubah menjadi tipe data float.
"""

col_to_int = ['earliest_cr_line', 'open_acc', 'pub_rec', 'total_acc', 'acc_now_delinq']
col_to_float = ['revol_bal', 'loan_amnt', 'desc_is_na']

df[col_to_int] = df[col_to_int].astype(int)
df[col_to_float] = df[col_to_float].astype(float)

"""### 6.3. Remove Outliers

Pada tahap ini, dilakukan deteksi outlier dalam dataset numerik, diikuti dengan penggantian nilai outlier dengan NaN, dan penghapusan baris yang mengandung nilai NaN. Tindakan ini bertujuan untuk membersihkan data dari outlier sehingga data yang digunakan untuk analisis atau pemodelan yang lebih konsisten.

> Memilih kolom-kolom dengan tipe data float dari DataFrame dan membuat DataFrame subset yang hanya berisi kolom-kolom tersebut.
"""

cols = df.select_dtypes(float).columns
df_sub = df.loc[:, cols]

"""> Menghasilkan boxplot yang menunjukkan distribusi batas atas dan batas bawah yang digunakan untuk mendeteksi outlier dalam DataFrame."""

Q1 = df_sub.quantile(0.25)
Q3 = df_sub.quantile(0.75)
IQR=Q3-Q1

outliers = ~((df_sub < (Q1 - 1.5 * IQR)) | (df_sub > (Q3 + 1.5 * IQR)))

plt.figure(figsize=(10, 5))
sns.boxplot(data=outliers)
plt.title('Visualisasi Deteksi Outlier')
plt.ylabel('Batas Atas dan Bawah')
plt.tight_layout()
plt.show()

"""> Membersihkan outlier dari DataFrame dengan mengganti nilai-nilai outlier dengan NaN dan menghapus baris-baris yang mengandung NaN dalam kolom-kolom yang dipilih."""

df.loc[:, cols] = df_sub.where(outliers, np.nan)
df.dropna(subset=cols, inplace=True)

"""> Menampilkan visualisasi data menggunakan boxplot setelah menghapus outlier."""

plt.figure(figsize=(10, 5))
sns.boxplot(data=df[cols])
plt.title('Visualisasi Data Setelah Menghapus Outlier')
plt.ylabel('Nilai')
plt.tight_layout()
plt.show()

"""### 6.4. Encode Data

Pada tahap ini, dilakukan konversi variabel kategorikal dalam dataset menjadi representasi numerik yang sesuai agar dapat dimengerti dan diproses oleh algoritma machine learning. Tahap ini melibatkan penggunaan teknik One-Hot-Encoding guna menyesuaikan data kategorikal menjadi format yang dapat dipahami oleh model machine learning.

#### 6.4.1. Define Target

Pada tahap ini, target variabel (`loan_status`) dalam dataset didefinisikan, dipersiapkan, dan dikonversi dari representasi teks menjadi representasi numerik menggunakan LabelEncoder.

> Membuat sebuah objek LabelEncoder yang dapat digunakan untuk melakukan encoding pada label kelas.
"""

le = LabelEncoder()

"""> Melakukan encoding pada kolom `loan_status`."""

df.loan_status = le.fit_transform(df.loan_status)

"""> Mencetak daftar kelas unik yang telah dipelajari oleh objek LabelEncoder."""

print(le.classes_)

"""> Mengubah label kelas dalam bentuk teks menjadi representasi numerik menggunakan objek LabelEncoder."""

print(le.transform(['Charged Off', 'Fully Paid']))

"""#### 6.4.2. Convert Categorical Variables

Pada tahap ini, variabel kategorikal dalam DataFrame diubah menjadi representasi numerik yang dapat digunakan untuk analisis lebih lanjut, terutama dalam konteks pemodelan data.

> Memilih kolom-kolom dalam DataFrame yang memiliki tipe data objek dan menampilkan lima baris pertama.
"""

df.select_dtypes(include=object).head()

"""> Mencetak nilai unik dari kolom `term` dalam DataFrame."""

print(df['term'].unique())

"""> Mengubah kolom 'term' dari representasi teks menjadi representasi numerik (dalam bulan)."""

df['term'] = np.where(df['term'] == " 36 months", 36,
             np.where(df['term'] == " 60 months", 60,
             df['term']))
df['term'] = df['term'].astype(int)
print(df['term'].unique())

"""#### 6.4.3. Create Transformer

Pada tahap ini, dilakukan preprocessing data pada DataFrame df dan hasil preprocessing disatukan dengan variabel target `loan_status` sehingga data siap untuk analisis lanjutan atau pembuatan model machine learning. Proses tersebut melibatkan penearapan transformasi pada kolom-kolom dalam DataFrame. Transformasi yang ditentukan adalah one-hot encoding untuk kolom-kolom kategorikal dan penskalaan min-max untuk kolom-kolom numerik.

> Mendapatkan daftar kolom-kolom dalam DataFrame yang memiliki tipe data objek.
"""

categories = df.select_dtypes(include=[object]).columns.tolist()

"""> Mendapatkan daftar kolom-kolom dalam DataFrame yang memiliki tipe data numerik."""

numerics = df.drop(['loan_status'], axis=1).select_dtypes(include=np.number).columns.tolist()

"""> Menginisialisasi objek dari kelas OneHotEncoder dan MinMaxScaler."""

ohe = OneHotEncoder(sparse_output=False)
scaler = MinMaxScaler()

"""> Menggunakan objek ColumnTransformer untuk menerapkan serangkaian transformasi pada kolom-kolom dalam DataFrame."""

ct = ColumnTransformer(
    [("onehotencoder", ohe, categories),
     ("minmaxscaler", scaler, numerics)],
    remainder = 'passthrough'
)

ct.set_output(transform='pandas')
processed_df = ct.fit_transform(df.drop(['loan_status'], axis=1))

"""> Menggabungkan DataFrame yang telah diproses `processed_df` dengan kolom `loan_status` dari DataFrame asli `df`."""

processed_df = pd.concat([processed_df, df['loan_status']], axis=1)

"""> Menghitung jumlah nilai unik dari variabel target `loan_status` dalam DataFrame `processed_df`."""

processed_df.loan_status.value_counts()

"""### 6.5. Train Test Split

Pada tahap ini, data dibagi menjadi subset latih dan uji, distribusi kelas dalam variabel target diperiksa, dan fitur serta target disiapkan untuk pelatihan dan pengujian model machine learning.

> Membagi DataFrame `processed_df` menjadi dua subset dengan proporsi sebesar 20% dari data dialokasikan untuk data uji, sedangkan 80% sisanya dialokasikan untuk menjadi data latih.
"""

train, test = train_test_split(processed_df, test_size=0.2, random_state=42)

"""> Mencetak dimensi (jumlah baris dan kolom) dari DataFrame train dan test."""

print(train.shape)
print(test.shape)

"""> Menghitung jumlah kemunculan setiap nilai unik dalam kolom `loan_status` dalam DataFrame test."""

test.loan_status.value_counts()

"""> Menghitung jumlah kemunculan setiap nilai unik dalam kolom `loan_status` dalam DataFrame train."""

train.loan_status.value_counts()

"""> Membagi dataset latih (train) dan dataset uji (test) menjadi dua bagian terpisah yang terdiri dari fitur (X_train dan X_test) dan variabel target (y_train dan y_test)."""

X_train, y_train = train.drop('loan_status', axis=1), train.loan_status
X_test, y_test = test.drop('loan_status', axis=1), test.loan_status

"""> Mencetak dimensi (jumlah baris dan kolom) dari DataFrame train dan test."""

print(X_train.shape)
print(y_train.shape)

"""## 7. Modelling

Pada tahap ini, proses pembangunan dan penyesuaian model dilakukan berdasarkan data yang tersedia untuk tujuan analisis, prediksi, atau pengambilan keputusan. Model-model ini digunakan untuk mengidentifikasi pola, hubungan, atau tren dalam data, serta untuk membuat prediksi atau estimasi berdasarkan informasi yang ada. Langkah-langkah dalam proses modelling meliputi pemilihan model yang sesuai, pelatihan model menggunakan data latih, evaluasi kinerja model menggunakan data uji, dan penyesuaian model untuk meningkatkan kinerja dan akurasi.
"""

ann_clf = tf.keras.models.Sequential([
    tf.keras.layers.Dense(256, activation='relu', input_shape=[X_train.shape[1]]),
    tf.keras.layers.Dropout(0.2),
    tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.Dropout(0.2),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dropout(0.2),
    tf.keras.layers.Dense(32, activation='relu'),
    tf.keras.layers.Dropout(0.2),
    tf.keras.layers.Dense(1, activation='sigmoid')
])

ann_clf.compile(
    optimizer='adam',
    loss='binary_crossentropy',
    metrics=['accuracy']
)

history = ann_clf.fit(
    x=X_train,
    y=y_train,
    epochs=10,
    batch_size=256,
    validation_data=(X_test, y_test)
)

key = ['RandomForestClassifier',
       'GradientBoostingClassifier',
       'AdaBoostClassifier',
       'XGBClassifier',
       'LogisticRegression',
       'KNeighborsClassifier',
       'NeuralNetwork']

value = [RandomForestClassifier(n_estimators=100, random_state=0, max_depth=None),
         GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42),
         AdaBoostClassifier(),
         xgb.XGBClassifier(random_state=42, booster="gbtree"),
         LogisticRegression(max_iter=1000),
         KNeighborsClassifier(n_neighbors=2, weights='uniform', algorithm='auto'),
         ann_clf]

models = dict(zip(key, value))

accuracy = pd.DataFrame(columns=['Accuracy Score'], index=key)

for name, algo in models.items():

  model = algo
  model.fit(X_train,y_train)

  if name == 'NeuralNetwork':
    predict = (ann_clf.predict(X_test) > 0.5).astype("int32")
  else:
    predict = model.predict(X_test)

  accuracy.loc[name] = accuracy_score(y_test, predict)

accuracy

"""> Menampilkan hasil visualisasi perbandingan skor akurasi untuk setiap model."""

plt.figure(figsize=(10, 5))

ax = sns.barplot(data=accuracy, x="Accuracy Score", y=accuracy.index, hue=accuracy.index, legend=False, orient='y')

for i, container in enumerate(ax.containers):
    ax.bar_label(container, fontsize=10)

ax.set_title('Perbandingan Skor Akurasi untuk Setiap Model')
plt.xlabel('Model')
plt.ylabel('Accuracy Score')
plt.xlim(0, 1)

plt.show()

"""> **Important Inferences**  
1. Mayoritas algoritma berhasil mencapai tingkat akurasi sebesar 81%, dengan pencapaian tertinggi terdapat pada algoritma AdaBoost yang mencapai 81.59%. Hal ini menunjukkan bahwa mayoritas dari model-model tersebut mampu mengklasifikasikan dengan benar sekitar 81% kasus pinjaman.

## 8. Evaluation

Pada tahap ini, dilakukan proses pengukuran kinerja dan akurasi model yang telah dibangun berdasarkan data yang digunakan untuk pelatihan. Tujuan dari evaluasi adalah untuk mengevaluasi seberapa baik model tersebut dapat memprediksi atau mengeneralisasi pola dari data baru yang belum pernah dilihat sebelumnya. Dalam hal ini, evaluasi akan dilkaukan dengan menggunakan Confusion Matrix dan ROC Curve and AUC. Implementasi confusion matrix dilakukan dengan melibatkan penggunaan matriks yang tercantum dalam classification report, seperti akurasi, presisi, recall, dan F-1 Score. Tentunya, evaluasi yang baik membantu memastikan bahwa model dapat memberikan hasil yang optimal.

### 8.1. Confusion Matrix

> Inisialisasi dictionary untuk menyimpan confusion matrix. Lalu, menghitung dan menyimpan confusion matrix untuk setiap algoritma.
"""

confusion_matrices = {}

for name, algo in models.items():
    confusion_matrices[name] = confusion_matrix(y_test, predict)

"""> Menampilkan hasil visualisasi Confussion Matrix."""

plt.figure(figsize=(18, 15))
for i, (name, matrix) in enumerate(confusion_matrices.items(), 1):
    plt.subplot(3, 3, i)
    sns.heatmap(matrix, annot=True, cmap='Blues', fmt='g')
    plt.title(f'Confusion Matrix untuk Algoritma {name}')
    plt.xlabel('Predicted')
    plt.ylabel('Actual')
plt.tight_layout()
plt.show()

""">  Inisialisasi dictionary untuk menyimpan laporan klasifikasi. Lalu, menghitung dan menyimpan laporan klasifikasi untuk setiap algoritma.

#### 8.1.1. Classification Report
"""

classification_reports = {}

for name, algo in models.items():

    report = classification_report(y_test, predict)
    classification_reports[name] = report

"""> Menampilkan laporan klasifikasi untuk setiap algoritma."""

for name, report in classification_reports.items():
    print(f"Classification Report untuk Algoritma {name}:\n{report}\n")

"""> **Important Inferences**  
1. Terdapat ketidakseimbangan kelas di mana terdapat lebih banyak kasus `Fully Paid` (kelas 1) daripada kasus `Charged Off` (kelas 0).
2. Seluruh model menunjukkan recall yang sangat tinggi (1.00) untuk kelas `Fully Paid` (kelas 1), yang menujukkan bahwa seluruh model hampir secara sempurna mengidentifikasi pinjaman yang baik.
3. Presisi untuk kelas `Charged Off` rendah (sekitar 0.55) untuk semua model. Ini menunjukkan bahwa model mungkin salah mengklasifikasikan banyak pinjaman yang buruk (charged off) sebagai pinjaman yang baik (fully paid).

### 8.2. ROC Curve and AUC

> Membuat DataFrame yang akan digunakan untuk menyimpan nilai AUC (Area Under the ROC Curve) dari setiap model yang dievaluasi.
"""

auc_scores = pd.DataFrame(columns=['AUC'], index=key)

"""> Melakukan evaluasi model dengan menggunakan kurva ROC (Receiver Operating Characteristic)."""

plt.figure(figsize=(8, 6))

for name, algo in models.items():
    model = algo
    model.fit(X_train, y_train)

    if name == 'NeuralNetwork':
        predict = ann_clf.predict(X_test)
    else:
        predict = model.predict_proba(X_test)[:, 1]

    fpr, tpr, thresholds = roc_curve(y_test, predict)
    roc_auc = auc(fpr, tpr)

    auc_scores.loc[name] = roc_auc

    plt.plot(fpr, tpr, label=f'{name} (AUC = {roc_auc:.2f})')

plt.plot([0, 1], [0, 1], color='navy', linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.0])

plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc="lower right")

plt.show()

"""> Menampilkan nilai AUC (Area Under the ROC Curve) dari setiap model yang dievaluasi."""

auc_scores

plt.figure(figsize=(10, 5))

ax = sns.barplot(data=auc_scores, x="AUC", y=auc_scores.index, hue=auc_scores.index, legend=False, orient='y')

for i, container in enumerate(ax.containers):
    ax.bar_label(container, fontsize=10)

ax.set_title('Perbandingan Skor AUC untuk Setiap Model')
plt.xlabel('Model')
plt.ylabel('AUC Score')
plt.xlim(0, 1)

plt.show()

""">**Important Inferences**
1. Seluruh model kecuali KNeighborsClassifier memiliki skor AUC yang serupa sekitar 0.7. Ini menunjukkan kemampuan moderat untuk membedakan antara pinjaman yang baik dan buruk.
2. LogisticRegression memiliki AUC tertinggi (0.702475). Ini menunjukkan kinerja yang sedikit lebih baik dalam membedakan jenis pinjaman.
3. KNeighborsClassifier memiliki AUC yang jauh lebih rendah (0.559735). Ini menunjukkan kinerja yang buruk dalam membedakan jenis pinjaman.

## 9. Kesimpulan

### 9.1. Kesimpulan

Dengan demikian, dapat disimpulkan bahwa model risiko kredit cenderung memiliki kinerja yang lebih baik dalam mengidentifikasi pinjaman yang baik daripada yang buruk. Kecenderungan ini dipengaruhi oleh ketidakseimbangan antara kelas dalam data yang digunakan untuk pelatihan model. Selain itu, Skor AUC menunjukkan bahwa model memiliki kemampuan yang cukup baik dalam menilai risiko kredit. Dengan meningkatkan skor AUC dan mempertimbangkan metrik lain, lending company (LC) dapat mengembangkan model yang lebih akurat dan sesuai dengan kebutuhan bisnis.

### 9.2. Saran

1. **Data Balancing**  
Teknik pengelolaan ketidakseimbangan data dapat diterapkan untuk meratakan proporsi kasus positif dan negatif dalam dataset pelatihan.
2. **Tuning Model**  
Proses penyetelan model dapat dilakukan untuk menyesuaikan hiperparameter algoritma guna meningkatkan kinerja terutama pada kelas minoritas, seperti kasus `Charged Off`.
3. **Business Recommendation**  
Dapat dilakukan pembuatan business recommendation berdasarkan data yang tersedia untuk mengembangkan potensi bisnis.

## 10. Referensi

[1]	G. Carlone, Introduction to Credit Risk. CRC Press, 2020.

[2]	N. Arora and P. D. Kaur, “A Bolasso based consistent feature selection enabled random forest classification algorithm: An application to credit risk assessment,” Applied Soft Computing, vol. 86, p. 105936, Jan. 2020, doi: 10.1016/j.asoc.2019.105936.

[3]	K. Peterdy, “Credit Risk,” Corporate Finance Institute. Accessed: Mar. 24, 2024. [Online]. Available: https://corporatefinanceinstitute.com/resources/commercial-lending/credit-risk/

[4]	G. Razis and S. Mitropoulos, “An integrated approach for the banking intranet/extranet information systems: the interoperability case,” International Journal of Business and Systems Research, vol. 1, no. 1, p. 1, 2022, doi: 10.1504/ijbsr.2022.10031295.

[5]	S. E. R, “Building a Random Forest Model: A Step-by-Step Guide,” Analytics Vidhya, Jun. 17, 2021. https://www.analyticsvidhya.com/blog/2021/06/understanding-random-forest/ (accessed Mar. 24, 2024).

[6]	A. Saini, “Gradient Boosting: A Step-by-Step Guide,” Analytics Vidhya, Sep. 20, 2021. https://www.analyticsvidhya.com/blog/2021/09/gradient-boosting-algorithm-a-complete-guide-for-beginners/ (accessed Mar. 24, 2024).

[7]	“AdaBoost Algorithm in Machine Learning,” AlmaBetter, Apr. 12, 2023. Accessed: Mar. 24, 2024. [Online]. Available: https://www.almabetter.com/bytes/tutorials/data-science/adaboost-algorithm.

[8]	“XGBoost Algorithm in Machine Learning,” AlmaBetter, Apr. 12, 2023. Accessed: Mar. 24, 2024. [Online]. Available: https://www.almabetter.com/bytes/tutorials/data-science/xgboost-algorithm    

[9]	guest_blog, “What Is XGBoost and How Does It Improve Machine Learning?,” Analytics Vidhya, Sep. 06, 2018. https://www.analyticsvidhya.com/blog/2018/09/an-end-to-end-guide-to-understand-the-math-behind-xgboost/ (accessed Mar. 24, 2024).

[10]	“What is Logistic Regression?,” Statistics Solutions, Dec. 21, 2010. https://www.statisticssolutions.com/free-resources/directory-of-statistical-analyses/what-is-logistic-regression/ (accessed Mar. 25, 2024).

[11]	A. Christopher, “K-Nearest Neighbor - The Startup - Medium,” The Startup, Feb. 03, 2021. Accessed: Mar. 25, 2024. [Online]. Available: https://medium.com/swlh/k-nearest-neighbor-ca2593d7a3c4    

[12]	J. Chen, “What Is a Neural Network?,” Investopedia, Sep. 12, 2006. Accessed: Mar. 25, 2024. [Online]. Available: https://www.investopedia.com/terms/n/neuralnetwork.asp
"""